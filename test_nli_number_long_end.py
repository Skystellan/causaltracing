import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import random

# ==============================================================================
# 1. æ ¸å¿ƒé€»è¾‘ç”Ÿæˆå™¨ (ä¿æŒåŸå°ä¸åŠ¨)
# ==============================================================================

def get_interval_case():
    age = random.randint(20, 59)
    decade = (age // 10) * 10
    wrong_decade = decade + 10
    if random.random() > 0.5:
        return {"type": "1. Interval Logic (Age)", "premise": f"The suspect is currently {age} years old.", "hypothesis": f"The suspect is in his {decade}s.", "expected": "Entailment"}
    else:
        return {"type": "1. Interval Logic (Age)", "premise": f"The suspect is currently {age} years old.", "hypothesis": f"The suspect is in his {wrong_decade}s.", "expected": "Contradiction"}

def get_decimal_precision_case():
    base = round(random.uniform(1, 10), 5)
    distorted = base + 0.00001
    return {"type": "2. High Precision Decimals", "premise": f"The scientific measurement was recorded precisely at {base:.5f} units.", "hypothesis": f"The scientific measurement was recorded precisely at {distorted:.5f} units.", "expected": "Contradiction"}

def get_large_number_format_case():
    val = random.randint(1, 9)
    half = random.choice([0, 5])
    num_str = f"{val},{half}00,000"
    word_str = f"{val}.{half} million"
    wrong_word_str = f"{val + 1}.{half} million"
    if random.random() > 0.5:
        return {"type": "3. Large Number Formats", "premise": f"The company profit hit {num_str} dollars last year.", "hypothesis": f"The company profit hit {word_str} dollars last year.", "expected": "Entailment"}
    else:
        return {"type": "3. Large Number Formats", "premise": f"The company profit hit {num_str} dollars last year.", "hypothesis": f"The company profit hit {wrong_word_str} dollars last year.", "expected": "Contradiction"}

def get_dense_info_case():
    metrics = ["Length", "Width", "Height", "Weight", "Depth"]
    random.shuffle(metrics)
    vals = [random.randint(10, 50) for _ in range(3)]
    premise_parts = [f"{metrics[i]} is {vals[i]}" for i in range(3)]
    premise = ", ".join(premise_parts) + "."
    target_idx = 1 
    target_metric = metrics[target_idx]
    original_val = vals[target_idx]
    if random.random() > 0.5:
        return {"type": "4. Dense Sentence Retrieval", "premise": f"Measurement details: {premise}", "hypothesis": f"Measurement details: {target_metric} is {original_val}.", "expected": "Entailment"}
    else:
        return {"type": "4. Dense Sentence Retrieval", "premise": f"Measurement details: {premise}", "hypothesis": f"Measurement details: {target_metric} is {original_val + 1}.", "expected": "Contradiction"}

def get_inequality_case():
    score = random.randint(80, 99)
    threshold = score - random.randint(5, 10)
    return {"type": "5. Inequality Logic (>)", "premise": f"The student achieved a score of {score} in the exam.", "hypothesis": f"The student achieved a score of more than {threshold} in the exam.", "expected": "Entailment"}

def get_chinese_numeral_case():
    map_cn = {1: "ä¸€", 2: "äºŒ", 3: "ä¸‰", 4: "å››", 5: "äº”", 6: "å…­", 7: "ä¸ƒ", 8: "å…«", 9: "ä¹", 10: "å"}
    val = random.randint(1, 10)
    cn_val = map_cn[val]
    return {"type": "6. Cross-Lingual (Chinese)", "premise": f"There are {val} apples on the table.", "hypothesis": f"There are {cn_val} apples on the table.", "expected": "Entailment"}

# ==============================================================================
# 2. ä¸Šä¸‹æ–‡å¡«å……å¼•æ“ (Context Injection Engine)
# ==============================================================================

def inject_context(tokenizer, core_case, target_total_length=512):
    """
    å°† core_case ä¸­çš„çŸ­ premise åŒ…è£…åˆ°é•¿æ–‡æœ¬ä¸­ã€‚
    æˆ‘ä»¬æŠŠæ ¸å¿ƒäº‹å®æ”¾åœ¨ Filler æ–‡æœ¬çš„ã€æœ«å°¾ã€‘ï¼Œè¿™é€šå¸¸æ˜¯æœ€è€ƒéªŒæ¨¡å‹æŠ—å¹²æ‰°èƒ½åŠ›çš„ä½ç½®ã€‚
    """
    short_premise = core_case['premise']
    hypothesis = core_case['hypothesis']
    
    # 1. å‡†å¤‡ä¸€æ®µè¶³å¤Ÿé•¿çš„ Filler Text (æ— å…³èƒŒæ™¯)
    # è¿™æ®µæ–‡æœ¬æ˜¯å…³äºæ•°æ®ç§‘å­¦çš„åºŸè¯ï¼Œæ²¡æœ‰ä»»ä½•æ•°å­—ï¼Œæ—¨åœ¨ç¨€é‡Šæ³¨æ„åŠ›
    filler_base = (
        "In the modern era of big data and artificial intelligence, processing vast amounts of unstructured text "
        "has become a critical challenge for neural networks. Traditional models often struggle with long-range dependencies, "
        "forgetting information that appeared early in the sequence. Transformers have revolutionized this field "
        "by utilizing self-attention mechanisms that allow the model to weigh the importance of different tokens "
        "regardless of their positional distance. However, even with these advancements, distinguishing subtle details "
        "embedded within a sea of irrelevant context remains a stress test for model robustness. "
        "Researchers continuously develop new architectures like sparse attention and linear complexity models "
        "to handle context windows extending beyond thousands of tokens. Data quality, preprocessing, and "
        "fine-tuning strategies are equally important. "
    )
    long_filler = filler_base * 8 # é‡å¤å¤šæ¬¡ä»¥ç¡®ä¿è¶³å¤Ÿé•¿
    
    # 2. è®¡ç®—é¢„ç®—
    # DeBERTa è¾“å…¥ç»“æ„: [CLS] Premise [SEP] Hypothesis [SEP]
    # æˆ‘ä»¬éœ€è¦ç¡®ä¿æ€»é•¿åº¦ä¸è¶… 512ï¼Œå¦åˆ™ core fact å¯èƒ½ä¼šè¢«æˆªæ–­
    
    # å…ˆæŠŠæ ¸å¿ƒéƒ¨åˆ†ç¼–ç 
    hyp_ids = tokenizer.encode(hypothesis, add_special_tokens=False)
    core_prem_ids = tokenizer.encode(short_premise, add_special_tokens=False)
    
    # é¢„ç•™ç‰¹æ®Š token çš„ä½ç½® (CLS, SEP, SEP) çº¦ 3-4 ä¸ª
    special_tokens_count = 4 
    
    # è®¡ç®— Filler æœ€å¤šèƒ½æœ‰å¤šå°‘ä¸ª token
    max_filler_tokens = target_total_length - len(hyp_ids) - len(core_prem_ids) - special_tokens_count
    
    if max_filler_tokens <= 0:
        return short_premise # æ ¸å¿ƒå¤ªé•¿äº†ï¼Œæ— æ³•å¡«å……
    
    # 3. æˆªå– Filler
    filler_ids = tokenizer.encode(long_filler, add_special_tokens=False)
    # å–å‰ max_filler_tokens ä¸ª
    selected_filler_ids = filler_ids[:max_filler_tokens]
    selected_filler_text = tokenizer.decode(selected_filler_ids, skip_special_tokens=True)
    
    # 4. ç»„è£…é•¿ Premise
    # æ ¼å¼: [æ— å…³åºŸè¯] + [å…³é”®è¯æç¤º] + [æ ¸å¿ƒäº‹å®]
    # è¿™æ ·æ ¸å¿ƒäº‹å®ä¼šå‡ºç°åœ¨å¤§çº¦ç¬¬ 400-500 ä¸ª token çš„ä½ç½®
    long_premise = f"{selected_filler_text} [KEY RECORD]: {short_premise}"
    
    return long_premise

# ==============================================================================
# 3. æ‰¹é‡å‹åŠ›æµ‹è¯•ä¸»ç¨‹åº
# ==============================================================================

def run_context_stress_test(samples_per_type=20):
    # 1. è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if torch.backends.mps.is_available(): device = torch.device("mps")
    
    model_name = "MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli"
    print(f"Loading model: {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)
    
    # 2. æ³¨å†Œç”Ÿæˆå™¨
    generators = [
        get_interval_case,
        get_decimal_precision_case,
        get_large_number_format_case,
        get_dense_info_case,
        get_inequality_case,
        get_chinese_numeral_case
    ]
    
    stats = {}
    
    print("\n" + "="*120)
    print(f"{'Type':<30} | {'Tokens':<6} | {'Core Premise (Buried at End)':<35} | {'Hypothesis':<25} | {'Exp':<5} | {'Pred':<5}")
    print("="*120)

    for gen_func in generators:
        # åˆå§‹åŒ–ç»Ÿè®¡
        temp = gen_func()
        t_name = temp['type']
        stats[t_name] = {"correct": 0, "total": 0}
        
        for _ in range(samples_per_type):
            # A. è·å–çŸ­æ ¸å¿ƒé€»è¾‘
            core_case = gen_func()
            
            # B. æ³¨å…¥é•¿ä¸Šä¸‹æ–‡ (æ ¸å¿ƒæ­¥éª¤)
            # æˆ‘ä»¬è¯·æ±‚å¡«æ»¡ 512 çš„çª—å£
            long_premise = inject_context(tokenizer, core_case, target_total_length=512)
            
            # C. ç¼–ç ä¸éªŒè¯é•¿åº¦
            inputs = tokenizer(
                long_premise, 
                core_case['hypothesis'], 
                truncation=True, 
                max_length=512, 
                return_tensors="pt"
            ).to(device)
            
            actual_tokens = inputs['input_ids'].shape[1]
            
            # D. æ¨ç†
            with torch.no_grad():
                outputs = model(**inputs)
            
            pred_id = torch.argmax(outputs.logits, dim=1).item()
            pred_label = model.config.id2label[pred_id]
            
            # E. åˆ¤å®š
            is_correct = (pred_label.lower() == core_case['expected'].lower())
            
            stats[t_name]['total'] += 1
            if is_correct: stats[t_name]['correct'] += 1
            
            # æ‰“å°å‰ 2 ä¸ªä½œä¸ºç¤ºä¾‹ï¼Œè¯æ˜ç¡®å®æ˜¯é•¿æ–‡æœ¬ä¸”æ ¸å¿ƒåœ¨æœ«å°¾
            if stats[t_name]['total'] <= 2:
                # åªæ‰“å° Premise çš„æœ€å 40 ä¸ªå­—ç¬¦ï¼Œè¯æ˜å‰é¢éƒ½æ˜¯åºŸè¯ï¼Œæ ¸å¿ƒåœ¨æœ€å
                p_snippet = "..." + long_premise[-40:] 
                h_snippet = core_case['hypothesis'][:25]
                icon = "âœ…" if is_correct else "âŒ"
                print(f"{t_name:<30} | {actual_tokens:<6} | {p_snippet:<35} | {h_snippet:<25} | {core_case['expected'][:4]:<5} | {pred_label[:4]:<5} {icon}")

    # ==============================================================================
    # 4. æœ€ç»ˆæŠ¥å‘Š
    # ==============================================================================
    print("\n" + "="*80)
    print("ğŸ“Š Final Robustness Report (Long Context Window ~512 Tokens)")
    print("="*80)
    
    all_passed = True
    for t_name, data in stats.items():
        acc = (data['correct'] / data['total']) * 100
        print(f"{t_name:<30}: {acc:.1f}% ({data['correct']}/{data['total']})")
        if acc < 90: all_passed = False
            
    print("="*80)
    print("\nğŸ’¡ Conclusion:")
    if all_passed:
        print(">> STRONG EVIDENCE: The model maintains STATIC SEMANTIC capabilities (Intervals, Precision, Formats)")
        print("   even when the information is buried at the end of a full 512-token window.")
        print("   This confirms that 'Needle-in-a-Haystack' retrieval is solved for DeBERTa-v3.")
    else:
        print(">> MIXED RESULTS: Some capabilities degraded under long context.")

if __name__ == "__main__":
    # æ¯ç§ç±»å‹æµ‹ 20 ä¸ªæ ·æœ¬ï¼Œæ€»è®¡ 120 æ¬¡æ¨ç†
    run_context_stress_test(samples_per_type=20)